{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "408ac139-9ac8-4dab-acb7-e6c85f7ef875",
   "metadata": {},
   "source": [
    "# 1.0 Training the Model and save model in file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b237ef18-992d-456e-a31c-1e7ebae96525",
   "metadata": {},
   "source": [
    "### 1.1 Setting up the env + the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e751110-48b0-41c1-a397-8b6721a760fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.7/site-packages (7.6.5)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.1.3)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (1.0.2)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (3.5.2)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (7.30.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets) (6.5.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.5.1)\n",
      "Requirement already satisfied: importlib-metadata<5 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (4.8.2)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: argcomplete>=1.12.3 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (1.12.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (6.1)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets) (7.1.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=4.0.0->ipywidgets) (59.4.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.9.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets) (4.2.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets) (6.4.6)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5->ipykernel>=4.5.1->ipywidgets) (3.6.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (21.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.5.4)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (0.3)\n",
      "Requirement already satisfied: pyzmq>=13 in /opt/conda/lib/python3.7/site-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (22.3.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (6.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.8.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.12.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (1.5.0)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.9)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (4.1.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.7.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (2.21)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (21.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (0.5.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets) (3.0.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86dd98d9-3d10-4593-9966-df51c76f5f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MNIST Model on cpu\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "# Training settings\n",
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d55b934a-2d75-4af9-8920-bfb9c004fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad706a-e863-4938-87ec-410e8aec1929",
   "metadata": {},
   "source": [
    "### 1.2 Defining the nn.Module function, set up each training and testing loop with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb11e7c8-9e99-4ecc-8fee-b78ecf922a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (l1): Linear(in_features=784, out_features=520, bias=True)\n",
       "  (l2): Linear(in_features=520, out_features=320, bias=True)\n",
       "  (l3): Linear(in_features=320, out_features=240, bias=True)\n",
       "  (l4): Linear(in_features=240, out_features=120, bias=True)\n",
       "  (l5): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "\n",
    "model = Net()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653c29a-1a92-4132-93f0-17a33db0e53e",
   "metadata": {},
   "source": [
    "### 1.3 Parameterize the learning rate for hyper parameter training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "396ba89d-3aaf-41b8-93e1-19bef33a0bca",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "learning_rate = \"0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0719f1ac-8c9f-4d8e-9e09-a49423342a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no longer needed to be defined as this is a hyper paramter that gets passed in. \n",
    "learning_rate = float(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9842d0-6cc4-4d2f-b210-b2acb6203277",
   "metadata": {},
   "source": [
    "### 1.4 Train and Test Function for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7cd24c37-aec4-4105-ac25-da52d16c4701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate for MNIST Model is 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)\n",
    "\n",
    "print(f'Learning Rate for MNIST Model is {learning_rate}\\n')\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += criterion(output, target).item()\n",
    "        # get the index of the max\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
    "          f'({100. * correct / len(test_loader.dataset):.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c9996-5ac3-4d87-9f4f-bf514cd4b238",
   "metadata": {},
   "source": [
    "### Execute Training and Testing to Verify Accuracy *SKIP IF JUST PREDICTING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67ef9e4d-9b5e-455d-94c1-7e118f79406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 | Batch Status: 0/60000 (0%) | Loss: 2.304031\n",
      "Train Epoch: 1 | Batch Status: 640/60000 (1%) | Loss: 2.301750\n",
      "Train Epoch: 1 | Batch Status: 1280/60000 (2%) | Loss: 2.302143\n",
      "Train Epoch: 1 | Batch Status: 1920/60000 (3%) | Loss: 2.297227\n",
      "Train Epoch: 1 | Batch Status: 2560/60000 (4%) | Loss: 2.290313\n",
      "Train Epoch: 1 | Batch Status: 3200/60000 (5%) | Loss: 2.284306\n",
      "Train Epoch: 1 | Batch Status: 3840/60000 (6%) | Loss: 2.271504\n",
      "Train Epoch: 1 | Batch Status: 4480/60000 (7%) | Loss: 2.267785\n",
      "Train Epoch: 1 | Batch Status: 5120/60000 (9%) | Loss: 2.217793\n",
      "Train Epoch: 1 | Batch Status: 5760/60000 (10%) | Loss: 2.194250\n",
      "Train Epoch: 1 | Batch Status: 6400/60000 (11%) | Loss: 2.105175\n",
      "Train Epoch: 1 | Batch Status: 7040/60000 (12%) | Loss: 1.681495\n",
      "Train Epoch: 1 | Batch Status: 7680/60000 (13%) | Loss: 1.240896\n",
      "Train Epoch: 1 | Batch Status: 8320/60000 (14%) | Loss: 1.251407\n",
      "Train Epoch: 1 | Batch Status: 8960/60000 (15%) | Loss: 1.255692\n",
      "Train Epoch: 1 | Batch Status: 9600/60000 (16%) | Loss: 0.944997\n",
      "Train Epoch: 1 | Batch Status: 10240/60000 (17%) | Loss: 0.753837\n",
      "Train Epoch: 1 | Batch Status: 10880/60000 (18%) | Loss: 0.577162\n",
      "Train Epoch: 1 | Batch Status: 11520/60000 (19%) | Loss: 0.998894\n",
      "Train Epoch: 1 | Batch Status: 12160/60000 (20%) | Loss: 0.530706\n",
      "Train Epoch: 1 | Batch Status: 12800/60000 (21%) | Loss: 0.654135\n",
      "Train Epoch: 1 | Batch Status: 13440/60000 (22%) | Loss: 0.799857\n",
      "Train Epoch: 1 | Batch Status: 14080/60000 (23%) | Loss: 0.645977\n",
      "Train Epoch: 1 | Batch Status: 14720/60000 (25%) | Loss: 0.348047\n",
      "Train Epoch: 1 | Batch Status: 15360/60000 (26%) | Loss: 0.419094\n",
      "Train Epoch: 1 | Batch Status: 16000/60000 (27%) | Loss: 0.477236\n",
      "Train Epoch: 1 | Batch Status: 16640/60000 (28%) | Loss: 0.237424\n",
      "Train Epoch: 1 | Batch Status: 17280/60000 (29%) | Loss: 0.380636\n",
      "Train Epoch: 1 | Batch Status: 17920/60000 (30%) | Loss: 0.256018\n",
      "Train Epoch: 1 | Batch Status: 18560/60000 (31%) | Loss: 0.495262\n",
      "Train Epoch: 1 | Batch Status: 19200/60000 (32%) | Loss: 0.400620\n",
      "Train Epoch: 1 | Batch Status: 19840/60000 (33%) | Loss: 0.401960\n",
      "Train Epoch: 1 | Batch Status: 20480/60000 (34%) | Loss: 0.384490\n",
      "Train Epoch: 1 | Batch Status: 21120/60000 (35%) | Loss: 0.290658\n",
      "Train Epoch: 1 | Batch Status: 21760/60000 (36%) | Loss: 0.308188\n",
      "Train Epoch: 1 | Batch Status: 22400/60000 (37%) | Loss: 0.390413\n",
      "Train Epoch: 1 | Batch Status: 23040/60000 (38%) | Loss: 0.366395\n",
      "Train Epoch: 1 | Batch Status: 23680/60000 (39%) | Loss: 0.408481\n",
      "Train Epoch: 1 | Batch Status: 24320/60000 (41%) | Loss: 0.297757\n",
      "Train Epoch: 1 | Batch Status: 24960/60000 (42%) | Loss: 0.252443\n",
      "Train Epoch: 1 | Batch Status: 25600/60000 (43%) | Loss: 0.242572\n",
      "Train Epoch: 1 | Batch Status: 26240/60000 (44%) | Loss: 0.239570\n",
      "Train Epoch: 1 | Batch Status: 26880/60000 (45%) | Loss: 0.397238\n",
      "Train Epoch: 1 | Batch Status: 27520/60000 (46%) | Loss: 0.202300\n",
      "Train Epoch: 1 | Batch Status: 28160/60000 (47%) | Loss: 0.499504\n",
      "Train Epoch: 1 | Batch Status: 28800/60000 (48%) | Loss: 0.335464\n",
      "Train Epoch: 1 | Batch Status: 29440/60000 (49%) | Loss: 0.178750\n",
      "Train Epoch: 1 | Batch Status: 30080/60000 (50%) | Loss: 0.183838\n",
      "Train Epoch: 1 | Batch Status: 30720/60000 (51%) | Loss: 0.232916\n",
      "Train Epoch: 1 | Batch Status: 31360/60000 (52%) | Loss: 0.260447\n",
      "Train Epoch: 1 | Batch Status: 32000/60000 (53%) | Loss: 0.167057\n",
      "Train Epoch: 1 | Batch Status: 32640/60000 (54%) | Loss: 0.532484\n",
      "Train Epoch: 1 | Batch Status: 33280/60000 (55%) | Loss: 0.183566\n",
      "Train Epoch: 1 | Batch Status: 33920/60000 (57%) | Loss: 0.226739\n",
      "Train Epoch: 1 | Batch Status: 34560/60000 (58%) | Loss: 0.231305\n",
      "Train Epoch: 1 | Batch Status: 35200/60000 (59%) | Loss: 0.228911\n",
      "Train Epoch: 1 | Batch Status: 35840/60000 (60%) | Loss: 0.121532\n",
      "Train Epoch: 1 | Batch Status: 36480/60000 (61%) | Loss: 0.130653\n",
      "Train Epoch: 1 | Batch Status: 37120/60000 (62%) | Loss: 0.222907\n",
      "Train Epoch: 1 | Batch Status: 37760/60000 (63%) | Loss: 0.173025\n",
      "Train Epoch: 1 | Batch Status: 38400/60000 (64%) | Loss: 0.075095\n",
      "Train Epoch: 1 | Batch Status: 39040/60000 (65%) | Loss: 0.320842\n",
      "Train Epoch: 1 | Batch Status: 39680/60000 (66%) | Loss: 0.161163\n",
      "Train Epoch: 1 | Batch Status: 40320/60000 (67%) | Loss: 0.414721\n",
      "Train Epoch: 1 | Batch Status: 40960/60000 (68%) | Loss: 0.230549\n",
      "Train Epoch: 1 | Batch Status: 41600/60000 (69%) | Loss: 0.268114\n",
      "Train Epoch: 1 | Batch Status: 42240/60000 (70%) | Loss: 0.278092\n",
      "Train Epoch: 1 | Batch Status: 42880/60000 (71%) | Loss: 0.271525\n",
      "Train Epoch: 1 | Batch Status: 43520/60000 (72%) | Loss: 0.187714\n",
      "Train Epoch: 1 | Batch Status: 44160/60000 (74%) | Loss: 0.148247\n",
      "Train Epoch: 1 | Batch Status: 44800/60000 (75%) | Loss: 0.188863\n",
      "Train Epoch: 1 | Batch Status: 45440/60000 (76%) | Loss: 0.202344\n",
      "Train Epoch: 1 | Batch Status: 46080/60000 (77%) | Loss: 0.154273\n",
      "Train Epoch: 1 | Batch Status: 46720/60000 (78%) | Loss: 0.237814\n",
      "Train Epoch: 1 | Batch Status: 47360/60000 (79%) | Loss: 0.223569\n",
      "Train Epoch: 1 | Batch Status: 48000/60000 (80%) | Loss: 0.099507\n",
      "Train Epoch: 1 | Batch Status: 48640/60000 (81%) | Loss: 0.159552\n",
      "Train Epoch: 1 | Batch Status: 49280/60000 (82%) | Loss: 0.041827\n",
      "Train Epoch: 1 | Batch Status: 49920/60000 (83%) | Loss: 0.255905\n",
      "Train Epoch: 1 | Batch Status: 50560/60000 (84%) | Loss: 0.208024\n",
      "Train Epoch: 1 | Batch Status: 51200/60000 (85%) | Loss: 0.076016\n",
      "Train Epoch: 1 | Batch Status: 51840/60000 (86%) | Loss: 0.189328\n",
      "Train Epoch: 1 | Batch Status: 52480/60000 (87%) | Loss: 0.086505\n",
      "Train Epoch: 1 | Batch Status: 53120/60000 (88%) | Loss: 0.086784\n",
      "Train Epoch: 1 | Batch Status: 53760/60000 (90%) | Loss: 0.163456\n",
      "Train Epoch: 1 | Batch Status: 54400/60000 (91%) | Loss: 0.200558\n",
      "Train Epoch: 1 | Batch Status: 55040/60000 (92%) | Loss: 0.150778\n",
      "Train Epoch: 1 | Batch Status: 55680/60000 (93%) | Loss: 0.058353\n",
      "Train Epoch: 1 | Batch Status: 56320/60000 (94%) | Loss: 0.220441\n",
      "Train Epoch: 1 | Batch Status: 56960/60000 (95%) | Loss: 0.200713\n",
      "Train Epoch: 1 | Batch Status: 57600/60000 (96%) | Loss: 0.231472\n",
      "Train Epoch: 1 | Batch Status: 58240/60000 (97%) | Loss: 0.148313\n",
      "Train Epoch: 1 | Batch Status: 58880/60000 (98%) | Loss: 0.036680\n",
      "Train Epoch: 1 | Batch Status: 59520/60000 (99%) | Loss: 0.198288\n",
      "Training time: 0m 11s\n",
      "===========================\n",
      "Test set: Average loss: 0.0028, Accuracy: 9431/10000 (94%)\n",
      "Testing time: 0m 12s\n",
      "Train Epoch: 2 | Batch Status: 0/60000 (0%) | Loss: 0.131088\n",
      "Train Epoch: 2 | Batch Status: 640/60000 (1%) | Loss: 0.180463\n",
      "Train Epoch: 2 | Batch Status: 1280/60000 (2%) | Loss: 0.143492\n",
      "Train Epoch: 2 | Batch Status: 1920/60000 (3%) | Loss: 0.108704\n",
      "Train Epoch: 2 | Batch Status: 2560/60000 (4%) | Loss: 0.107395\n",
      "Train Epoch: 2 | Batch Status: 3200/60000 (5%) | Loss: 0.037059\n",
      "Train Epoch: 2 | Batch Status: 3840/60000 (6%) | Loss: 0.269066\n",
      "Train Epoch: 2 | Batch Status: 4480/60000 (7%) | Loss: 0.186149\n",
      "Train Epoch: 2 | Batch Status: 5120/60000 (9%) | Loss: 0.102985\n",
      "Train Epoch: 2 | Batch Status: 5760/60000 (10%) | Loss: 0.217833\n",
      "Train Epoch: 2 | Batch Status: 6400/60000 (11%) | Loss: 0.194657\n",
      "Train Epoch: 2 | Batch Status: 7040/60000 (12%) | Loss: 0.180914\n",
      "Train Epoch: 2 | Batch Status: 7680/60000 (13%) | Loss: 0.139493\n",
      "Train Epoch: 2 | Batch Status: 8320/60000 (14%) | Loss: 0.193090\n",
      "Train Epoch: 2 | Batch Status: 8960/60000 (15%) | Loss: 0.277302\n",
      "Train Epoch: 2 | Batch Status: 9600/60000 (16%) | Loss: 0.040420\n",
      "Train Epoch: 2 | Batch Status: 10240/60000 (17%) | Loss: 0.145926\n",
      "Train Epoch: 2 | Batch Status: 10880/60000 (18%) | Loss: 0.229086\n",
      "Train Epoch: 2 | Batch Status: 11520/60000 (19%) | Loss: 0.042387\n",
      "Train Epoch: 2 | Batch Status: 12160/60000 (20%) | Loss: 0.031521\n",
      "Train Epoch: 2 | Batch Status: 12800/60000 (21%) | Loss: 0.034018\n",
      "Train Epoch: 2 | Batch Status: 13440/60000 (22%) | Loss: 0.018818\n",
      "Train Epoch: 2 | Batch Status: 14080/60000 (23%) | Loss: 0.109352\n",
      "Train Epoch: 2 | Batch Status: 14720/60000 (25%) | Loss: 0.112745\n",
      "Train Epoch: 2 | Batch Status: 15360/60000 (26%) | Loss: 0.108923\n",
      "Train Epoch: 2 | Batch Status: 16000/60000 (27%) | Loss: 0.038723\n",
      "Train Epoch: 2 | Batch Status: 16640/60000 (28%) | Loss: 0.139144\n",
      "Train Epoch: 2 | Batch Status: 17280/60000 (29%) | Loss: 0.131696\n",
      "Train Epoch: 2 | Batch Status: 17920/60000 (30%) | Loss: 0.083482\n",
      "Train Epoch: 2 | Batch Status: 18560/60000 (31%) | Loss: 0.181405\n",
      "Train Epoch: 2 | Batch Status: 19200/60000 (32%) | Loss: 0.115124\n",
      "Train Epoch: 2 | Batch Status: 19840/60000 (33%) | Loss: 0.288530\n",
      "Train Epoch: 2 | Batch Status: 20480/60000 (34%) | Loss: 0.058860\n",
      "Train Epoch: 2 | Batch Status: 21120/60000 (35%) | Loss: 0.146567\n",
      "Train Epoch: 2 | Batch Status: 21760/60000 (36%) | Loss: 0.102658\n",
      "Train Epoch: 2 | Batch Status: 22400/60000 (37%) | Loss: 0.049281\n",
      "Train Epoch: 2 | Batch Status: 23040/60000 (38%) | Loss: 0.076427\n",
      "Train Epoch: 2 | Batch Status: 23680/60000 (39%) | Loss: 0.111020\n",
      "Train Epoch: 2 | Batch Status: 24320/60000 (41%) | Loss: 0.116682\n",
      "Train Epoch: 2 | Batch Status: 24960/60000 (42%) | Loss: 0.172982\n",
      "Train Epoch: 2 | Batch Status: 25600/60000 (43%) | Loss: 0.197823\n",
      "Train Epoch: 2 | Batch Status: 26240/60000 (44%) | Loss: 0.148619\n",
      "Train Epoch: 2 | Batch Status: 26880/60000 (45%) | Loss: 0.119676\n",
      "Train Epoch: 2 | Batch Status: 27520/60000 (46%) | Loss: 0.071124\n",
      "Train Epoch: 2 | Batch Status: 28160/60000 (47%) | Loss: 0.079181\n",
      "Train Epoch: 2 | Batch Status: 28800/60000 (48%) | Loss: 0.106912\n",
      "Train Epoch: 2 | Batch Status: 29440/60000 (49%) | Loss: 0.039497\n",
      "Train Epoch: 2 | Batch Status: 30080/60000 (50%) | Loss: 0.287499\n",
      "Train Epoch: 2 | Batch Status: 30720/60000 (51%) | Loss: 0.167882\n",
      "Train Epoch: 2 | Batch Status: 31360/60000 (52%) | Loss: 0.111258\n",
      "Train Epoch: 2 | Batch Status: 32000/60000 (53%) | Loss: 0.059203\n",
      "Train Epoch: 2 | Batch Status: 32640/60000 (54%) | Loss: 0.017678\n",
      "Train Epoch: 2 | Batch Status: 33280/60000 (55%) | Loss: 0.103299\n",
      "Train Epoch: 2 | Batch Status: 33920/60000 (57%) | Loss: 0.082120\n",
      "Train Epoch: 2 | Batch Status: 34560/60000 (58%) | Loss: 0.078736\n",
      "Train Epoch: 2 | Batch Status: 35200/60000 (59%) | Loss: 0.121320\n",
      "Train Epoch: 2 | Batch Status: 35840/60000 (60%) | Loss: 0.165427\n",
      "Train Epoch: 2 | Batch Status: 36480/60000 (61%) | Loss: 0.092360\n",
      "Train Epoch: 2 | Batch Status: 37120/60000 (62%) | Loss: 0.055419\n",
      "Train Epoch: 2 | Batch Status: 37760/60000 (63%) | Loss: 0.060584\n",
      "Train Epoch: 2 | Batch Status: 38400/60000 (64%) | Loss: 0.201292\n",
      "Train Epoch: 2 | Batch Status: 39040/60000 (65%) | Loss: 0.061156\n",
      "Train Epoch: 2 | Batch Status: 39680/60000 (66%) | Loss: 0.143420\n",
      "Train Epoch: 2 | Batch Status: 40320/60000 (67%) | Loss: 0.156408\n",
      "Train Epoch: 2 | Batch Status: 40960/60000 (68%) | Loss: 0.147133\n",
      "Train Epoch: 2 | Batch Status: 41600/60000 (69%) | Loss: 0.381444\n",
      "Train Epoch: 2 | Batch Status: 42240/60000 (70%) | Loss: 0.099146\n",
      "Train Epoch: 2 | Batch Status: 42880/60000 (71%) | Loss: 0.159082\n",
      "Train Epoch: 2 | Batch Status: 43520/60000 (72%) | Loss: 0.111181\n",
      "Train Epoch: 2 | Batch Status: 44160/60000 (74%) | Loss: 0.029985\n",
      "Train Epoch: 2 | Batch Status: 44800/60000 (75%) | Loss: 0.097433\n",
      "Train Epoch: 2 | Batch Status: 45440/60000 (76%) | Loss: 0.029121\n",
      "Train Epoch: 2 | Batch Status: 46080/60000 (77%) | Loss: 0.029103\n",
      "Train Epoch: 2 | Batch Status: 46720/60000 (78%) | Loss: 0.164299\n",
      "Train Epoch: 2 | Batch Status: 47360/60000 (79%) | Loss: 0.125075\n",
      "Train Epoch: 2 | Batch Status: 48000/60000 (80%) | Loss: 0.079211\n",
      "Train Epoch: 2 | Batch Status: 48640/60000 (81%) | Loss: 0.320103\n",
      "Train Epoch: 2 | Batch Status: 49280/60000 (82%) | Loss: 0.065952\n",
      "Train Epoch: 2 | Batch Status: 49920/60000 (83%) | Loss: 0.029816\n",
      "Train Epoch: 2 | Batch Status: 50560/60000 (84%) | Loss: 0.023974\n",
      "Train Epoch: 2 | Batch Status: 51200/60000 (85%) | Loss: 0.094533\n",
      "Train Epoch: 2 | Batch Status: 51840/60000 (86%) | Loss: 0.053925\n",
      "Train Epoch: 2 | Batch Status: 52480/60000 (87%) | Loss: 0.207456\n",
      "Train Epoch: 2 | Batch Status: 53120/60000 (88%) | Loss: 0.118584\n",
      "Train Epoch: 2 | Batch Status: 53760/60000 (90%) | Loss: 0.115949\n",
      "Train Epoch: 2 | Batch Status: 54400/60000 (91%) | Loss: 0.102109\n",
      "Train Epoch: 2 | Batch Status: 55040/60000 (92%) | Loss: 0.017540\n",
      "Train Epoch: 2 | Batch Status: 55680/60000 (93%) | Loss: 0.148473\n",
      "Train Epoch: 2 | Batch Status: 56320/60000 (94%) | Loss: 0.189274\n",
      "Train Epoch: 2 | Batch Status: 56960/60000 (95%) | Loss: 0.053052\n",
      "Train Epoch: 2 | Batch Status: 57600/60000 (96%) | Loss: 0.067761\n",
      "Train Epoch: 2 | Batch Status: 58240/60000 (97%) | Loss: 0.006321\n",
      "Train Epoch: 2 | Batch Status: 58880/60000 (98%) | Loss: 0.159991\n",
      "Train Epoch: 2 | Batch Status: 59520/60000 (99%) | Loss: 0.020621\n",
      "Training time: 0m 11s\n",
      "===========================\n",
      "Test set: Average loss: 0.0017, Accuracy: 9660/10000 (97%)\n",
      "Testing time: 0m 12s\n",
      "Total Time: 0m 24s\n",
      "Model was trained on cpu!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    since = time.time()\n",
    "    for epoch in range(1, 3):\n",
    "        epoch_start = time.time()\n",
    "        train(epoch)\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
    "        test()\n",
    "        m, s = divmod(time.time() - epoch_start, 60)\n",
    "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
    "\n",
    "    m, s = divmod(time.time() - since, 60)\n",
    "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a9a4c-5419-4eb7-b5fa-fc4e21bb7856",
   "metadata": {},
   "source": [
    "### Saving the models to file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7855bdc1-5dc4-4e53-ae4e-40ad5e4e33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'model_weights_1.pth')\n",
    "torch.save(model, f'model_lr{learning_rate}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3279d8-b03a-4b3e-9aa0-9b119739ee47",
   "metadata": {},
   "source": [
    "# 2.0. Self-Contain Predict Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9821f6d-f2d8-464a-9245-8a032a6d8cfd",
   "metadata": {},
   "source": [
    "### Predict function with model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745a4fe9-0f70-469f-8da8-6462402a4b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting MNIST Model on cpu\n",
      "============================================\n",
      "Training MNIST Model on cpu\n",
      "============================================\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "from __future__ import print_function\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Predicting MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "# Declare nn.module class with model arch\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(784, 520)\n",
    "        self.l2 = nn.Linear(520, 320)\n",
    "        self.l3 = nn.Linear(320, 240)\n",
    "        self.l4 = nn.Linear(240, 120)\n",
    "        self.l5 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)  # Flatten the data (n, 1, 28, 28)-> (n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "\n",
    "# Load\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
    "\n",
    "model_load = Net()\n",
    "model = torch.load(f'model_lr{learning_rate}.pth')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# x is assume to be a tensor object that should be 784 element\n",
    "# TODO: need to figure out how to take a object and pass it into the api\n",
    "\n",
    "#predict function\n",
    "def predict(x):\n",
    "    x = torch.tensor(x)\n",
    "    x = x.to(device)\n",
    "    output = model(x)\n",
    "    prediction = output.data.max(1, keepdim=True)[1]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890cef4-7dfe-4259-99be-64dde9283097",
   "metadata": {},
   "source": [
    "### Invoking the prediction logic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ee2593d-c177-4eb0-ab50-8ffc7bdca8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy the input into array (4D)\n",
    "picture=[[[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3294, 0.7255,\n",
    "           0.6235, 0.5922, 0.2353, 0.1412, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8706, 0.9961,\n",
    "           0.9961, 0.9961, 0.9961, 0.9451, 0.7765, 0.7765, 0.7765, 0.7765,\n",
    "           0.7765, 0.7765, 0.7765, 0.7765, 0.6667, 0.2039, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2627, 0.4471,\n",
    "           0.2824, 0.4471, 0.6392, 0.8902, 0.9961, 0.8824, 0.9961, 0.9961,\n",
    "           0.9961, 0.9804, 0.8980, 0.9961, 0.9961, 0.5490, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0667, 0.2588, 0.0549, 0.2627, 0.2627,\n",
    "           0.2627, 0.2314, 0.0824, 0.9255, 0.9961, 0.4157, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.3255, 0.9922, 0.8196, 0.0706, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0863, 0.9137, 1.0000, 0.3255, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.5059, 0.9961, 0.9333, 0.1725, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.2314, 0.9765, 0.9961, 0.2431, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.5216, 0.9961, 0.7333, 0.0196, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0353,\n",
    "           0.8039, 0.9725, 0.2275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4941,\n",
    "           0.9961, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2941, 0.9843,\n",
    "           0.9412, 0.2235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0745, 0.8667, 0.9961,\n",
    "           0.6510, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.7961, 0.9961, 0.8588,\n",
    "           0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.1490, 0.9961, 0.9961, 0.3020,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.1216, 0.8784, 0.9961, 0.4510, 0.0039,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.5216, 0.9961, 0.9961, 0.2039, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.2392, 0.9490, 0.9961, 0.9961, 0.2039, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.4745, 0.9961, 0.9961, 0.8588, 0.1569, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.4745, 0.9961, 0.8118, 0.0706, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000],\n",
    "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "           0.0000, 0.0000, 0.0000, 0.0000]]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "098acf81-5311-4164-b1ae-78ffd62cb0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7]])\n"
     ]
    }
   ],
   "source": [
    "# Use predict on the array\n",
    "print(predict(picture))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adcd4b-6f22-4c17-991e-0d2e9e849124",
   "metadata": {},
   "source": [
    "#this logic grab some test data (batch1) into variable data. \n",
    "#this is not needed if we have a predict function\n",
    "\n",
    "\n",
    "# Training settings\n",
    "batch_size = 1\n",
    "\n",
    "# MNIST Dataset\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False)\n",
    "\n",
    "\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    result = predict(data)\n",
    "    print(result)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu:latest"
  },
  "kernelspec": {
   "display_name": "Pytorch (Local)",
   "language": "python",
   "name": "local-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
